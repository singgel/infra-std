# 内存虚拟化

## 虚拟地址

**为什么要有虚拟地址**

没有虚拟地址空间的时候，程序都是直接访问和操作物理内存。但会有一些问题：

1.  内存空间利用率的问题

各个进程对内存的使用会导致内存碎片化，当要用malloc分配一块很大的内存空间时，可能会出现虽然有足够多的空闲物理内存，却没有足够大的连续空闲内存这种情况，东一块西一块的内存碎片就被浪费掉了

2.  读写内存的安全性问题

物理内存本身是不限制访问的，任何地址都可以读写，而现代操作系统需要实现不同的页面具有不同的访问权限，例如只读的数据等等

3.  进程间的安全问题

各个进程之间没有独立的地址空间，一个进程由于执行错误指令或是恶意代码都可以直接修改其它进程的数据，甚至修改内核地址空间的数据，这是操作系统所不愿看到的

4.  内存读写的效率问题

当多个进程同时运行，需要分配给进程的内存总和大于实际可用的物理内存时，需要将其他程序暂时拷贝到硬盘当中，然后将新的程序装入内存运行。由于大量的数据频繁装入装出，内存的使用效率会非常低

通过虚拟地址访问内存有以下优势：

1.  程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的大内存缓冲区。
1.  程序可以使用一系列虚拟地址来访问大于可用物理内存的内存缓冲区。当物理内存的供应量变小时，内存管理器会将物理内存页保存到磁盘文件。
1.  不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改由另一进程使用的物理内存。

## 分段

程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（** ***Segmentation*** **）的形式把这些段分离出来。**

> 分段机制下，虚拟地址和物理地址是如何映射的？

分段机制下的虚拟地址由两部分组成，**段选择符**和**段内偏移量**。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cbae023f787249d4a92341d588973616~tplv-k3u1fbpfcp-zoom-1.image)

内存分段寻址方式

分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处：

-   第一个就是**内存碎片**的问题，确切来说是外部碎片。

<!---->

-   第二个就是**内存交换的效率低**的问题。

接下来，说说为什么会有这两个问题。

> 我们先来看看，分段为什么会产生内存碎片的问题？

将空间切成不同长度的分片以后，空间本身会碎片化（fragmented），随着时间推移，分配内存会变得比较困难。

举例：

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5e81472c50534dc9be574a5f21b617ba~tplv-k3u1fbpfcp-zoom-1.image)

解决外部内存碎片的问题就是**内存交换**。

可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。

这个内存交换空间，在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。

> 再来看看，分段为什么会导致内存交换效率低的问题？

对于多进程的系统来说，用分段的方式，内存碎片是很容易产生的，产生了内存碎片，那不得不重新 `Swap` 内存区域，这个过程会产生性能瓶颈。

因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。

所以，**如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**

为了解决内存分段的内存碎片和内存交换效率低的问题，就出现了内存分页。
## 分页

**页式存储管理是一种把** **主存** **按页分配的存储管理方式，主存-辅存间信息传送单位是定长的页。** 对比段式管理而言，因为管理的粒度更细致，所以造成内存页碎片的浪费也会小很多。而缺点也正好相反，由于页不是程序独立模块对应的逻辑实体，所以处理、保护和共享都不及段来得方便。同时也**因为页要比段小得多，在 Linux 下通常默认设置为 4KB，所以页在进行交换时，不会出现段交换那般卡顿。** 所以，页式存储管理方式会更加的受到欢迎，Linux 操作系统采用的就是页式存储管理方式。

页式存储管理方式使得加载程序的时候，不再需要一次性都把程序加载到内存中，而是在程序运行中需要用到的对应虚拟内存页里面的指令和数据时，再将其加载到内存中，这些操作由操作系统来完成。当 CPU 要读取特定的页，但却发现页的内容却没有加载时，就会触发一个来自 CPU 的**缺页错误（Page Fault）** 。此时操作系统会捕获这个错误，然后找到对应的页并加载到内存中。通过这种方式，使得我们可以运行哪些远大于实例物理内存的程序，但相对的执行效率也会有所下降。

通过虚拟存储器、内存交换、内存分页三个技术的结合。我们最终得到了一个不需要让程序员考虑实际的物理内存地址、大小和当前分配空间的程序运行环境。这些技术和方式对于程序员和程序的编译、链接过程而言都是透明的，印证了那句著名的话：所有计算机问题都可以通过插入一个中间层来解决。

页表管理机制中有两个很重要的概念：快表和多级页表。在分页内存管理中，很重要的两点是：

1.  虚拟地址到物理地址的转换要快。
1.  解决虚拟地址空间大，页表也会很大的问题。

### TLB

快表：为了解决虚拟地址到物理地址的转换速度，操作系统在 页表方案 基础之上引入了 快表 来加速虚拟地址到物理地址的转换。我们可以把块表理解为一种特殊的高速缓冲存储器（Cache），其中的内容是页表的一部分或者全部内容。作为页表的 Cache，它的作用与页表相似，但是提高了访问速率。由于采用页表做地址转换，读写内存数据时 CPU 要访问两次主存（多级页表不止两次）。有了快表，有时只要访问一次高速缓冲存储器，一次主存，这样可加速查找并提高指令执行速度。

使用快表之后的地址转换流程是这样的：

-   根据虚拟地址中的页号查快表；
-   如果该页在快表中，直接从快表中读取相应的物理地址；
-   如果该页不在快表中，就访问内存中的页表，再从页表中得到物理地址，同时将页表中的该映射表项添加到快表中；
-   当快表填满后，又要登记新页时，就按照一定的淘汰策略淘汰掉快表中的一个页。

看完了之后你会发现快表和我们平时经常在我们开发的系统使用的缓存（比如 Redis）很像，的确是这样的，**操作系统中的很多思想、很多经典的算法，都可以在我们日常开发使用的各种工具或者框架中找到它们的影子。**

### 多级页表

多级页表：**引入多级页表的主要目的是为了避免把全部页表一直放在内存中占用过多空间，特别是那些根本就不需要的页表就不需要保留在内存中。** 多级页表属于时间换空间的典型场景

为了提高内存的空间性能，提出了多级页表的概念；但是提到空间性能是以浪费时间性能为基础的（多次访问内存），因此为了补充损失的时间性能，提出了快表（即 TLB）的概念。不论是快表还是多级页表实际上都利用到了程序的**局部性原理**。

为了存储 64 位操作系统中 128 TiB 虚拟内存的映射数据，Linux 在 2.6.10 中引入了四层的页表辅助虚拟地址的转换。四级目录，分别是：

-   全局页目录项 PGD（*Page Global Directory*）；

<!---->

-   上层页目录项 PUD（*Page Upper Directory*）；

<!---->

-   中间页目录项 PMD（*Page Middle Directory*）；

<!---->

-   页表项 PTE（*Page Table Entry*）；

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dfbf122e0cbf46f6b7f34867e17bf061~tplv-k3u1fbpfcp-zoom-1.image)

### 虚实地址转换

同任何缓存设计一样，虚拟存储器系统必须有某种方法来判定一个虚拟页是否存放在物理主存的某个地方。如果存在，系统还必须确定这个虚拟页存放在哪个物理页中。如果物理主存不命中，系统必须判断这个虚拟页存放在磁盘的哪个位置中，并在物理主存中选择一个牺牲页，然后将目标虚拟页从磁盘拷贝到物理主存中，替换掉牺牲页。这些功能是由许多软硬件联合提供，包括操作系统软件，MMU（存储器管理单元）地址翻译硬件和一个存放在物理主存中的叫做页表（Page Table）的数据结构，页表将虚拟页映射到物理页。页表的本质就是一个页表条目（Page Table Entry，PTE）数组。

CPU 通过虚拟地址（Virtual Address，VA）来访问存储空间，这个虚拟地址在被送到存储器之前需要先转换成适当的物理地址。将一个虚拟地址转换为物理地址的任务叫做地址翻译（Address Translation）。就像异常处理一样，地址翻译需要 CPU 硬件和操作系统之间的紧密合作。比如：Linux 操作系统的交换空间（Swap Space）。如果当 CPU 寻址时发现虚拟地址找不到对应的物理地址，那么就会触发一个异常并挂起寻址错误的进程。在这个过程中，对其他进程没有任何影响。

**虚拟地址与物理地址之间的转换主要由 CPU 芯片上内嵌的存储器管理单元（Memory Management Unit，MMU）完成**，它是一个专用的硬件，利用存放在主存中的查询表（地址映射表）来动态翻译虚拟地址，该表的内容由操作系统管理。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f74b4c80d2e94167ac3fcde90fb898eb~tplv-k3u1fbpfcp-zoom-1.image)

### 缺页中断

地址映射过程中，若在页面中发现所要访问的页面不在内存中，则发生缺页中断。缺页中断 就是要访问的页不在主存，需要操作系统将其调入主存后再进行访问。在这个时候，被内存映射的文件实际上成了一个分页交换文件。当发生缺页中断时，如果当前内存中并没有空闲的页面，操作系统就必须在内存选择一个页面将其移出内存，以便为即将调入的页面让出空间。用来选择淘汰哪一页的规则叫做页面置换算法，我们可以把页面置换算法看成是淘汰页面的策略。

-   OPT 页面置换算法（最佳页面置换算法） ：理想情况，不可能实现，一般作为衡量其他置换算法的方法。
-   FIFO 页面置换算法（先进先出页面置换算法） : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。
-   LFU 页面置换算法（最少使用页面排序算法） ：LFU（Least Frequently Used）算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间T，当须淘汰一个页面时，选择现有页面中其T值最大的，即最近最久未使用的页面予以淘汰。
-   LRU 页面置换算法（最近未使用页面置换算法） : LRU（Least Recently Used）算法会让系统维护一个按最近一次访问时间排序的页面链表，链表首节点是最近刚刚使用过的页面，链表尾节点是最久未使用的页面。访问内存时，找到相应页面，并把它移到链表之首。缺页时，置换链表尾节点的页面。也就是说内存内使用越频繁的页面，被保留的时间也相对越长。
## Linux段页式管理

内存分段和内存分页并不是对立的，它们是可以组合起来在同一个系统中使用的，组合起来后，通常称为**段页式内存管理**。

那么，Linux 操作系统采用了哪种方式来管理内存呢？在回答这个问题前，我们得先看看 Intel 处理器的发展历史。

早期 Intel 的处理器从 80286 开始使用的是段式内存管理。但是很快发现，光有段式内存管理而没有页式内存管理是不够的，这会使它的 X86 系列会失去市场的竞争力。因此，在不久以后的 80386 中就实现了对页式内存管理。也就是说，80386 除了完成并完善从 80286 开始的段式内存管理的同时还实现了页式内存管理。

但是这个 80386 的页式内存管理设计时，没有绕开段式内存管理，而是建立在段式内存管理的基础上，这就意味着，**页式内存管理的作用是在由段式内存管理所映射而成的地址上再加上一层地址映射。**

由于此时由段式内存管理映射而成的地址不再是“物理地址”了，Intel 就称之为“线性地址”（也称虚拟地址）。于是，段式内存管理先将逻辑地址映射成线性地址，然后再由页式内存管理将线性地址映射成物理地址。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ba5c7127e66d46a88d8f41992ee8b261~tplv-k3u1fbpfcp-zoom-1.image)

了解完 Intel 处理器的发展历史后，我们再来说说 Linux 采用了什么方式管理内存？**Linux 内存主要采用的是页式内存管理，但同时也不可避免地涉及了段机制**。

这主要是上面 Intel 处理器发展历史导致的，因为 Intel X86 CPU 一律对程序中使用的地址先进行段式映射，然后才能进行页式映射。既然 CPU 的硬件结构是这样，Linux 内核也只好服从 Intel 的选择。

**但是事实上，Linux 内核所采取的办法是使段式映射的过程实际上不起什么作用。**

Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是一样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。

# 地址空间

在32位模式下它是一个4GB的内存地址块。在Linux系统中, 内核进程和用户进程所占的虚拟内存比例是1:3。

虚拟地址通过页表(Page Table)映射到物理内存，页表由操作系统维护并被处理器引用。内核空间在页表中拥有较高特权级，因此用户态程序试图访问这些页时会导致一个页错误(page fault)。在Linux中，内核空间是持续存在的，并且在所有进程中都映射到同样的物理内存。内核代码和数据总是可寻址，随时准备处理中断和系统调用。与此相反，用户模式地址空间的映射随进程切换的发生而不断变化。

Linux进程在虚拟内存中的标准内存段布局如下图所示：

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a44523c50064451faf2ad5d40b7a8e42~tplv-k3u1fbpfcp-zoom-1.image)

## 用户空间

### 分段存储

用户进程部分分段存储内容如下表所示(从高往低)

| 名称   | 存储内容                  |
| ---- | --------------------- |
| 栈    | 局部变量、函数参数、返回地址等       |
| 堆    | 动态分配的内存               |
| BSS段 | 未初始化或初值为0的全局变量和静态局部变量 |
| 数据段  | 已初始化且初值非0的全局变量和静态局部变量 |
| 代码段  | 可执行代码、字符串字面值、只读变量     |

进程运行过程中，代码指令根据流程依次执行，只需访问一次(当然跳转和递归可能使代码执行多次)；而数据通常需要访问多次，因此单独开辟空间以方便访问和节约空间。具体解释如下：

当程序被装载后，数据和指令分别映射到两个虚存区域。数据区对于进程而言可读写，而指令区对于进程只读。**两区的权限可分别设置为可读写和只读。以防止程序指令被有意或无意地改写。**

现代CPU具有极为强大的缓存(Cache)体系，程序必须尽量提高缓存命中率。**指令区和数据区的分离有利于提高程序的局部性。** 现代CPU一般数据缓存和指令缓存分离，故程序的指令和数据分开存放有利于提高CPU缓存命中率。

当系统中运行多个该程序的副本时，其指令相同，故内存中只须保存一份该程序的指令部分。若系统中运行数百进程，通过共享指令将节省大量空间(尤其对于有动态链接的系统)。其他只读数据如程序里的图标、图片、文本等资源也可共享。而每个副本进程的数据区域不同，它们是进程私有的。

此外，临时数据及需要再次使用的代码在运行时放入栈区中，生命周期短。全局数据和静态数据可能在整个程序执行过程中都需要访问，因此单独存储管理。堆区由用户自由分配和管理。

### 内存管理单元

前面我们提到，Linux进程可以划分为 5 个不同的内存区域，分别是：代码段、数据段、`BSS`、堆、栈，内核管理这些区域的方式是，**将这些内存区域抽象成**`vm_area_struct` **(VMA)的内存管理对象。**

`vm_area_struct`是描述进程地址空间的基本管理单元，它描述的是一段连续的、具有相同访问属性的虚存空间，该虚存空间的大小为物理内存页面的整数倍。

在进程的 task_struct结构中包含一个指向 mm_struct 结构的指针。进程的mm_struct 则包含装入的可执行映象信息以及进程的页目录指针pgd。该结构还包含有指向 vm_area_struct 结构的几个指针，每个 vm_area_struct 代表进程的一个虚拟地址区间。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0eb4259f14054318b5473608c2a41efa~tplv-k3u1fbpfcp-zoom-1.image)

进程虚拟地址示意图

上图是某个进程的虚拟内存简化布局以及相应的几个数据结构之间的关系。从图中可以看出，系统以虚拟内存地址的降序排列 vm_area_struct。在进程的运行过程中，Linux 要经常为进程分配虚拟地址区间，或者因为从交换文件中装入内存而修改虚拟地址信息，因此，vm_area_struct结构的访问时间就成了性能的关键因素。为此，除链表结构外，Linux 还利用 红黑树来组织 vm_area_struct。通过这种树结构，Linux 可以快速定位某个虚拟内存地址。

当进程利用系统调用动态分配内存时，Linux 首先分配一个 vm_area_struct 结构，并链接到进程的虚拟内存链表中，当后续的指令访问这一内存区间时，因为 Linux 尚未分配相应的物理内存，因此处理器在进行虚拟地址到物理地址的映射时会产生缺页异常，当 Linux 处理这一缺页异常时，就可以为新的虚拟内存区分配实际的物理内存。

## 内核空间

在 `x86 32` 位系统里，Linux 内核地址空间是指虚拟地址从 `0xC0000000` 开始到 `0xFFFFFFFF` 为止的高端内存地址空间，总计 `1G` 的容量， 包括了内核镜像、物理页面表、驱动程序等运行在内核空间

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5be32ab5871e4c80b6bf84f2689379a4~tplv-k3u1fbpfcp-zoom-1.image)

内核空间细分区域

### **直接映射区**

直接映射区 `Direct Memory Region`：从内核空间起始地址开始，最大`896M`的内核空间地址区间，为直接内存映射区。

直接映射区的896MB的「线性地址」直接与「物理地址」的前`896MB`进行映射，也就是说线性地址和分配的物理地址都是连续的。内核地址空间的线性地址`0xC0000001`所对应的物理地址为`0x00000001`，**它们之间相差一个偏移量**`PAGE_OFFSET = 0xC0000000`

该区域的线性地址和物理地址存在线性转换关系「线性地址 = `PAGE_OFFSET` + 物理地址」也可以用 `virt_to_phys()`函数将内核虚拟空间中的线性地址转化为物理地址。

### **高端内存线性地址空间**

内核空间线性地址从 896M 到 1G 的区间，容量 128MB 的地址区间是高端内存线性地址空间，为什么叫高端内存线性地址空间：

前面已经说过，内核空间的总大小 1GB，从内核空间起始地址开始的 896MB 的线性地址可以直接映射到物理地址大小为 896MB 的地址区间。

退一万步，即使内核空间的1GB线性地址都映射到物理地址，那也最多只能寻址 1GB 大小的物理内存地址范围。远低于现代内存条的容量。

所以，内核空间拿出了最后的 128M 地址区间，划分成下面三个高端内存映射区，以达到对整个物理地址范围的寻址（而在 64 位的系统上就不存在这样的问题了，因为可用的线性地址空间远大于可安装的内存）。

-   动态内存映射区

由内核函数vmalloc分配，内核地址连续；但物理地址不一定连续。

-   永久内存映射区

内核建立由高端内存页框到内核地址空间的长期映射。

-   固定映射区

留作特定用途。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/38a75f5cdaac471b8917ccef20593b11~tplv-k3u1fbpfcp-zoom-1.image)

内核空间物理内存映射
# 内存管理

## 页框管理

### Linux是如何组织物理内存的

-   node

目前计算机系统有两种体系结构：

1.  非一致性内存访问 NUMA（Non-Uniform Memory Access）意思是内存被划分为各个node，访问一个node花费的时间取决于CPU离这个node的距离。每一个cpu内部有一个本地的node，访问本地node时间比访问其他node的速度快
1.  一致性内存访问 UMA（Uniform Memory Access）也可以称为SMP（Symmetric Multi-Process）对称多处理器。意思是所有的处理器访问内存花费的时间是一样的。也可以理解整个内存只有一个node。

> 比较：UMA体系结构为访问内存的处理器提供了相同的总体延迟。当访问本地内存时，这不是很有用，因为等待时间是统一的。另一方面，在NUMA中，每个处理器都有其专用内存，从而消除了访问本地内存时的延迟。延迟随着处理器和存储器之间的距离的改变而改变（即不均匀）。但是与UMA架构相比，NUMA改善了性能。

-   zone

ZONE的意思是把整个物理内存划分为几个区域，每个区域有特殊的含义

-   page

代表一个物理页，在内核中一个物理页用一个struct page表示。

-   page frame

假设一个page的大小是4K的，内核会将整个物理内存分割成一个一个4K大小的物理页，而4K大小物理页的区域我们称为page frame

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a506f1e3e6e5459a9111a893428a0484~tplv-k3u1fbpfcp-zoom-1.image)

-   page frame num(pfn)

pfn是对每个page frame的编号。故物理地址和pfn的关系是：

`物理地址>>PAGE_SHIFT = pfn`

系统启动的时候，内核会将整个struct page映射到内核虚拟地址空间vmemmap的区域，所以我们可以简单的认为struct page的基地址是vmemmap，则：

vmemmap+pfn的地址就是此struct page对应的物理地址。

### **内存**管理区

> NUMA结构下, 每个处理器CPU与一个本地内存直接相连, 而不同处理器之间则通过总线进行进一步的连接, 因此相对于任何一个CPU访问本地内存的速度比访问远程内存的速度要快, 而Linux为了兼容NUMA结构, 把物理内存依照CPU的不同node分成簇, 一个CPU-node对应一个本地内存pgdata_t.

> 这样已经很好的表示物理内存了, 在一个理想的计算机系统中, 一个页框就是一个内存的分配单元, 可用于任何事情:存放内核数据, 用户数据和缓冲磁盘数据等等. 任何种类的数据页都可以存放在任页框中, 没有任何限制.

**但是Linux内核又把各个物理内存节点分成个不同的管理区域zone, 这是为什么呢?**

因为实际的计算机体系结构有硬件的诸多限制, 这限制了页框可以使用的方式. 尤其是, Linux内核必须处理80x86体系结构的两种硬件约束：

1.  ISA总线的直接内存存储DMA处理器有一个严格的限制 : 他们只能对RAM的前16MB进行寻址
1.  在具有大容量RAM的现代32位计算机中, CPU不能直接访问所有的物理地址, 因为线性地址空间太小, 内核不可能直接映射所有物理内存到线性地址空间

因此Linux内核对不同区域的内存需要采用不同的管理方式和映射方式, 内核将物理地址划分成用zone_t表示的不同地址区域

对于x86机器，管理区(内存区域)类型如下分布

| 类型           | 区域           |
| ------------ | ------------ |
| ZONE_DMA     | 0~16MB       |
| ZONE_NORMAL  | 16MB~895MB   |
| ZONE_HIGHMEM | 896MB~物理内存结束 |

而由于32位系统中, Linux内核虚拟地址空间只有1G, 而0~895M这个896MB被用于DMA和直接映射, 剩余的物理内存被称为高端内存. 那内核是如何借助剩余128MB高端内存地址空间是如何实现访问所有物理内存？

当内核想访问高于896MB物理地址内存时，从0xF8000000 ~ 0xFFFFFFFF地址空间范围内找一段相应大小空闲的逻辑地址空间，借用一会。借用这段逻辑地址空间，建立映射到想访问的那段物理内存（即填充内核PTE页面表），临时用一会，用完后归还。这样别人也可以借用这段地址空间访问其他物理内存，实现了使用有限的地址空间，访问所有所有物理内存。

### 分区页框分配器

> 处理对连续页框的分配请求

页框分配在内核里的机制我们叫做分区页框分配器(zoned page frame allocator)，在linux系统中，分区页框分配器管理着所有物理内存，无论你是内核还是进程，都需要请求分区页框分配器，这时才会分配给你应该获得的物理内存页框。当你所拥有的页框不再使用时，你必须释放这些页框，让这些页框回到管理区页框分配器当中。

管理区分配器**接受动态内存分配与释放的请求**，它**首先**从每CPU页框高速缓存中请求页框，若**无法满足**才从**伙伴系统**中请求分配。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/862437fca47540fd8910b3000b09a8c2~tplv-k3u1fbpfcp-zoom-1.image)

内核中根据不同的分配需求有6个函数接口来请求页框，最终都会调用到__alloc_pages_nodemask。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cf5d931b96b844f0b742fb1483d55e5c~tplv-k3u1fbpfcp-zoom-1.image)

### 伙伴系统算法

在实际应用中，经常需要分配一组连续的页框，而**频繁地申请和释放不同大小的连续页框，必然导致在已分配页框的内存块中分散了许多小块的空闲页框**。这样，即使有足够的页框是空闲的，其他需要分配连续页框的应用也很难得到满足 **（外碎片问题）。**

为了避免出现这种情况，Linux内核中**引入了伙伴系统算法(buddy system)** 。把所有的**空闲页框分组为11个块链表** 存于数组free_area，每个块链表分别**包含大小为1，2，4，8，16，32，64，128，256，512和1024个连续页框的页框块**。最大可以申请1024个连续页框，对应4MB大小的连续内存。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8b39971f732c4868bef455a0126d57ee~tplv-k3u1fbpfcp-zoom-1.image)

假设要**申请一个256个页框的块**，先从256个页框的链表中查找空闲块，**如果没有**，就去512个页框的链表中找，找到了则**将页框块分为2个256个页框的块**，一个分配给应用，另外一个移到256个页框的链表中。如果512个页框的链表中仍没有空闲块，继续向1024个页框的链表查找，如果仍然没有，则返回错误。

上面的逆过程就是页框块的释放过程，页框块在释放时，会主动将两个连续的页框块合并为一个较大的页框块。这样被合并的块被称为伙伴块，伙伴块满足以下条件：

-   两个块大小相同；
-   两个块地址连续；
-   两个块必须是同一个大块中分离出来的；

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5ad9f66b36dc4c08ac0801d703d6235a~tplv-k3u1fbpfcp-zoom-1.image)

free_area[k]的元素的free_list字段是双向循环链表的头，标识了所有页数量为2^k的空闲块，这个双向循环链表集中了大小为2^k页的空闲块的起始页框的页描述符。

一个大小为2^k页的空闲块的描述符的**private字段**存放了块的order为k，靠这个字段内核可以**确定其伙伴是否空闲**。

### 每CPU页框高速缓存

内核经常请求和释放单个页框。为了提升系统性能，请求单个或释放单个页框时，内核在使用伙伴算法之前多添了一个步骤，即每CPU页框高速缓存。

每个内存管理区定义了一个“每CPU”页框高速缓存，所有“每CPU”高速缓存包含一些预先分配的页框，它们被用于满足本地CPU 发出的单个页内存请求。

## 内存区管理

### Slab分配器

#### 设计原因

在Linux中，伙伴分配器（buddy allocator）是以页为单位管理和分配内存。但在内核中的需求却以字节为单位（在内核中面临频繁的结构体内存分配问题）。假如我们需要动态申请一个内核结构体（占 20 字节），若仍然分配一页内存，这将严重浪费内存。那么该如何分配呢？**slab 分配器专为小内存分配而生**，由Sun公司的一个雇员`Jeff Bonwick`在`Solaris 2.4`中设计并实现。slab分配器分配内存**以字节为单位**，基于伙伴分配器的大内存进一步细分成小内存分配。换句话说，slab 分配器仍然从 Buddy 分配器中申请内存，之后自己对申请来的内存细分管理。

除了提供小内存外，slab 分配器的第二个任务是**维护常用对象的缓存**。对于内核中使用的许多结构，初始化对象所需的时间可能等于或超过为其分配空间的成本。当创建一个新的slab 时，许多对象将被打包到其中并使用构造函数（如果有）进行初始化。释放对象后，它会保持其初始化状态，这样可以快速分配对象。

> 举例来说, 为管理与进程关联的文件系统数据, 内核必须经常生成`struct fs_struct`的新实例. 此类型实例占据的内存块同样需要经常回收(在进程结束时). 换句话说, 内核趋向于非常有规律地分配并释放大小为`sizeof(fs_struct)`的内存块. slab分配器将释放的内存块保存在一个内部列表中. 并不马上返回给伙伴系统. 在请求为该类对象分配一个新实例时, 会使用最近释放的内存块。这有两个优点. 首先, 由于内核不必使用伙伴系统算法, 处理时间会变短. 其次, 由于该内存块仍然是”新”的，因此其仍然驻留在CPU硬件缓存的概率较高.

SLAB分配器的最后一项任务是**提高CPU硬件缓存的利用率**。 如果将对象包装到SLAB中后仍有剩余空间，则将剩余空间用于为SLAB着色（后节中详述）。 SLAB着色是一种尝试使不同SLAB中的对象使用CPU硬件缓存中不同行的方案。 通过将对象放置在SLAB中的不同起始偏移处，对象可能会在CPU缓存中使用不同的行，从而有助于确保来自同一SLAB缓存的对象不太可能相互刷新。

####

#### slab实现原理

**对象object**

object是slab内存分配器对外提供的申请内存的基本单位。slab内存分配器从buddy system申请了buddy之后，会将其拆分成一个个object，并缓存在kmem cache实例的cpu_cache中，用户申请内存时，其实获取的就是一个个object。一旦object缓存耗尽，就会重新从buddy system申请slab，并将其拆分成object，放入内存池。

由于slab的缓存特性，slab 分配器从 buddy 分配器中获取的物理内存称为 `内存缓存`（与 CPU 的硬件缓存进行区别，下文都称为缓存），使用结构体`struct kmem_cache`（定义在`include/linux/slab_def.h`文件中）描述。

每个缓存节点在内存中维护称为slab的连续页块，这些页面被切成小块，用于缓存数据结构和对象。 `kmem_cache`的 `kmem_cache_node` 成员记录了该kmem_cache 下的所有 slabs 列表。形成的结构如下图所示。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5674fb463cf3418bac730c636c7e2dd8~tplv-k3u1fbpfcp-zoom-1.image)

`kmem_cache_node` 定义在`mm/slab.h`文件中，如下所示

```
struct kmem_cache_node {

    spinlock_t list_lock;



#ifdef CONFIG_SLAB

    struct list_head slabs_partial; /* partial list first, better asm code */

    struct list_head slabs_full;

    struct list_head slabs_free;

    unsigned long total_slabs;  /* 所有 slab list 的长度 */

    unsigned long free_slabs;   /* 仅 free slab list 的长度*/

    unsigned long free_objects;

    ...

#endif



};
```

kmem_cache_node 记录了3种slab：

-   slabs_full ：已经完全分配的 slab

<!---->

-   slabs_partial： 部分分配的slab

<!---->

-   slabs_free：空slab，或者没有对象被分配

以上3个链表保存的是slab 描述符，Linux kernel 使用 `struct page` 来描述一个slab。**单个slab可以在slab链表之间移动**，例如如果一个半满slab被分配了对象后变满了，就要从 slabs_partial 中被删除，同时插入到 slabs_full 中去。

#### 本地 CPU 空闲对象链表

在 kmem_cache 结构体中还有一个成员`struct array_cache __percpu *cpu_cache;`。其用来为每个CPU维护一个空间对象链表。这能带来如下好处：

-   提高硬件缓存的命中率。每个CPU都有它们自己的硬件高速缓存（在多核CPU下，通常 L1/L2 为 CPU 单独占有，L2/L3为所有CPU核共享的）。当在此CPU上释放对象时，又再次申请一个相同大小的对象时，原对象很可能还在这个CPU的硬件高速缓存中。内核为每个CPU维护一个这样的链表，当需要新的对象时，会优先尝试从当前CPU的本地CPU空闲对象链表获取相应大小的对象，这样就能更快地分配对象。

<!---->

-   减少锁的竞争。假设多个CPU同时申请一个大小的slab，这时候如果没有本地CPU空闲对象链表，就会导致分配流程是互斥的，需要上锁，就导致分配效率低。

`struct array_cache`定义在文件`mm/slab.c`文件中，如下所示：

```
struct array_cache {

    /* 可用对象数目 */

    unsigned int avail;

    /* 可拥有的最大对象数目，和kmem_cache中一样 */

    unsigned int limit;

    /* 同kmem_cache，要转移进本地高速缓存或从本地高速缓存中转移出去的对象的数量 */

    unsigned int batchcount;

    /* 是否在收缩后被访问过 */

    unsigned int touched;

    /* 伪数组，初始没有任何数据项，之后会增加并保存释放的对象指针 */

    void *entry[];    /*

};
```

本地CPU空闲对象链表在系统初始化完成后是一个空的链表，只有释放对象时才会将对象加入这个链表。链表对象个数是有限制的（最大值就是limit），链表数超过这个值时，会将 batchcount 个数的对象返回到所有CPU共享的空闲对象链表(也是这样一个结构)中。

#### 所有 CPU 共享的空闲对象链表

在 `struct kmem_cache_node`结构体中同样存在一个 `array_cache`的成员 `struct array_cache *shared`，其构成一个所有 CPU 共享的缓存。原理和本地CPU空闲对象链表一样，唯一的区别就是所有CPU都可以从这个链表中获取对象.

在此基础上，一个常规的对象申请流程是这样的：

-   内核首先会从本地 CPU 空闲对象链表中尝试获取一个对象用于分配：如果失败，则检查所有CPU共享的空闲对象链表链表中是否存在，并且空闲链表中是否存在空闲对象，若有就转移 batchcount 个空闲对象到本地 CPU空闲对象链表中；

<!---->

-   如果第 1 步失败，就尝试从 SLAB中分配；这时如果还失败，kmem_cache会尝试从页框分配器中获取一组连续的页框建立一个新的SLAB，然后从新的SLAB中获取一个对象。

对象释放流程如下：

-   首先会先将对象释放到本地CPU空闲对象链表中，如果本地CPU空闲对象链表中对象过多，kmem_cache 会将本地CPU空闲对象链表中的batchcount个对象移动到所有CPU共享的空闲对象链表链表中，
-   如果所有CPU共享的空闲对象链表链表的对象也太多了，kmem_cache也会把所有CPU共享的空闲对象链表链表中batchcount个数的对象移回它们自己所属的SLAB中，
-   这时如果SLAB中空闲对象太多，kmem_cache会整理出一些空闲的SLAB，将这些SLAB所占用的页框释放回页框分配器中。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d30766f9836b448cb61f16a362a1e382~tplv-k3u1fbpfcp-zoom-1.image)

#### slab着色

在slab中，相同大小的对象倾向于存放在硬件高速缓存内部相同的cache line中（高速缓存读物理内存的位置不是任意的，而是固定的）。由此产生的问题是，不同slab中，相同大小的对象很可能最终映射到相同的cache line中，当进行针对这两个对象的读操作时，就出现了两个对象在cache line和RAM之间来回不停切换的现象，更糟糕的是，剩下的一些cache line可能正在无所事事。着色的主要目的就是避免类似的现象发生。

同一硬件高速缓存行可以映射 RAM 中多个不同的块，相同大小的对象倾向于存放在高速缓存内相同的偏移量处。在不同 slab 内具有相同偏移量的对象最终很可能映射到同一高速缓存行中。而使用 slab 分配器的对象通常是频繁使用的小对象，高速缓存的硬件可能因此而花费内存周期在同一高速缓存行与 RAM 内存单元之间来来往往的传送两个对象。

如下例：假设 cache 行为 32Bytes ， CPU 包含 512 个 cache 行（缓存大小 16K ）。

假设对象 A,B 均为 32B ，且 A 的地址从 0 开始， B 的地址从 16K 开始，则根据组相联或直接相联映射方式（全相联方式很少使用）， A,B 对象很可能映射到 cache 的第 0 行 ，此时，如果 CPU 交替的访问 A,B 各 50 次，每一次访问 cache 第 0 行都失效，从而需要从内存传送数据。而 slab 着色就是为解决该问题产生的，不同的颜色代表了不同的起始对象偏移量，对于 B 对象，如果将其位置偏移向右偏移 32B ，则其可能会被映射到 cache 的第 1 行上，这样交替的访问 A,B 各 50 次，只需要 2 次内存访问即可。

这里的偏移量就代表了 slab 着色中的一种颜色，不同的颜色代表了不同的偏移量，尽量使得不同的对象的对应到不同的硬件高速缓存行上，以最大限度的提高效率。
#### 通用和专用缓存

通过命令`sudo cat /proc/slabinfo`可查看系统当前 slab 使用情况。以`vm_area_struct`结构体为例，当前系统已分配了 13014 个`vm_area_struct`缓存，每个大小为 216 字节，其中 active 的有 12392 个。

```
[root@VM-8-9-centos]# cat /proc/slabinfo

slabinfo - version: 2.1

# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab> : tunables <limit> <batchcount> <sharedfactor> : slabdata <active_slabs> <num_slabs> <sharedavail>

...

vm_area_struct     12392  13014    216   18    1 : tunables    0    0    0 : slabdata    723    723      0

mm_struct            184    200   1600   20    8 : tunables    0    0    0 : slabdata     10     10      0

shared_policy_node   5015   5015     48   85    1 : tunables    0    0    0 : slabdata     59     59      0

numa_policy           15     15    264   15    1 : tunables    0    0    0 : slabdata      1      1      0

radix_tree_node    15392  17234    584   14    2 : tunables    0    0    0 : slabdata   1231   1231      0

idr_layer_cache      240    240   2112   15    8 : tunables    0    0    0 : slabdata     16     16      0

kmalloc-8192          39     44   8192    4    8 : tunables    0    0    0 : slabdata     11     11      0

kmalloc-4096         117    144   4096    8    8 : tunables    0    0    0 : slabdata     18     18      0

kmalloc-2048         458    528   2048   16    8 : tunables    0    0    0 : slabdata     33     33      0

kmalloc-1024        1399   1424   1024   16    4 : tunables    0    0    0 : slabdata     89     89      0

kmalloc-512          763    800    512   16    2 : tunables    0    0    0 : slabdata     50     50      0

kmalloc-256         3132   3376    256   16    1 : tunables    0    0    0 : slabdata    211    211      0

kmalloc-192         2300   2352    192   21    1 : tunables    0    0    0 : slabdata    112    112      0

kmalloc-128         1376   1376    128   32    1 : tunables    0    0    0 : slabdata     43     43      0

kmalloc-96          1596   1596     96   42    1 : tunables    0    0    0 : slabdata     38     38      0

kmalloc-64         16632  20800     64   64    1 : tunables    0    0    0 : slabdata    325    325      0

kmalloc-32          1664   1664     32  128    1 : tunables    0    0    0 : slabdata     13     13      0

kmalloc-16          4608   4608     16  256    1 : tunables    0    0    0 : slabdata     18     18      0

kmalloc-8           4096   4096      8  512    1 : tunables    0    0    0 : slabdata      8      8      0

...
```

可以看到，系统中存在的含有具体名字的 slab 我们称其为 专用 slab，用来为特定结构体分配内存，如 vm_area_struct、mm_struct 等。而其他形如 kmalloc-xxx 的 slab，我们称其为通用型 slab，用来满足分配通用内存。

**kmalloc** 是基于slab 分配器的，`kmalloc()` 分配的虚拟地址范围在内核空间的「直接内存映射区」。按字节为单位虚拟内存，一般用于分配小块内存，释放内存对应于 `kfree` ，可以分配连续的物理内存。函数原型在 `<linux/kmalloc.h>` 中声明，**一般情况下在驱动程序中都是调用** ` kmalloc()  `**来给数据结构分配内存 。**

为什么要分专用和通用 slab ？ **最直观的一个原因就是通用 slab 会造成内存浪费**：出于 slab 管理的方便，每个 slab 管理的对象大小都是一致的，当我们需要分配一个处于 64-96字节中间大小的对象时，就必须从保存 96 字节的 slab 中分配。而对于专用的 slab，其管理的都是同一个结构体实例，申请一个就给一个恰好内存大小的对象，这就可以充分利用空间。

## 非连续内存区管理

### vmalloc

伙伴系统也好、slab技术也好，从内存管理理论角度而言目的基本是一致的，它们都是为了防止“碎片”。不过度片又分为外部碎片和内部碎片之说，所谓内部碎片是说系统为了满足一小段内存区（连续）的需要，不得不分配了一大区域连续内存给它，从而形成了空间浪费；外部碎片是指系统虽有足够的内存，但倒是分散的碎片，没法知足对大块“连续内存”的需求。不管何种碎片都是系统有效利用内存的障碍。slab分配器使得一个页面内包含的众多小块内存可独立被分配使用，避免了内部碎片，节约了空闲内存。伙伴系统把内存块按大小分组管理，必定程度上减轻了外部碎片的危害，由于页框分配不再盲目，而是按照大小依次有序进行，不过伙伴关系只是减轻了外部碎片，但并未完全消除。

因此避免外部碎片的最终思路仍是落到了如何利用不连续的内存块组合成“看起来很大的内存块”——这里的状况很相似于用户空间分配虚拟内存，内存逻辑上连续，其实映射到并不必定连续的物理内存上。Linux内核借用了这个技术，容许内核程序在内核地址空间中分配虚拟地址，一样也利用页表（内核页表）将虚拟地址映射到分散的内存页上。以此完美地解决了内核内存使用中的外部碎片问题。内核提供vmalloc函数分配内核虚拟内存，该函数不一样于kmalloc，它能够分配较kmalloc大得多的内存空间（可远大于128K，但必须是页大小的倍数），但相比kmalloc来讲，vmalloc需要对内核虚拟地址进行重映射，必须更新内核页表，所以分配效率上要低一些（用时间换空间）

由get_free_page或kmalloc函数所分配的连续内存都限于物理映射区域，因此它们返回的内核虚拟地址和实际物理地址仅仅是相差一个偏移量（PAGE_OFFSET），你能够很方便的将其转化为物理内存地址，同时内核也提供了virt_to_phys（）函数将内核虚拟空间中的物理映射区地址转化为物理地址。物理内存映射区中的地址与内核页表是有序对应的，系统中的每一个物理页面均可以找到它对应的内核虚拟地址（在物理内存映射区中的）。

而vmalloc分配的地址则限于vmalloc_start与vmalloc_end之间。每一块vmalloc分配的内核虚拟内存都对应一个vm_struct结构体（可别和vm_area_struct搞混，那是进程虚拟内存区域的结构），不一样的内核虚拟地址被4k大小的空闲区间隔，以防止越界。与进程虚拟地址的特性同样，这些虚拟地址与物理内存没有简单的位移关系，必须经过内核页表才可转换为物理地址或物理页。它们有可能还没有被映射，在发生缺页时才真正分配物理页面。

**vmalloc和kmalloc的区别**

1，kmalloc对应于kfree, 分配的内存处于3GB~high memory之间，这段内核空间与物理内存的映射一一对应，可以分配连续的物理内存；

vmalloc对应于vfree, 分配的内存在VMALLOC_START ~ 4GB之间，分配连续的虚拟内存，但物理上不一定连续。

2，kmalloc分配内存是基于slab, 因此slab的一些特性包括着色，对齐等都具备，性能较好。物理地址和逻辑地址都是连续的。

3，最主要的区别是分配大小的问题，比如你需要28个字节，那一定用kmalloc。而vmalloc用于大块内存的分配。

尽管仅仅在某些情况下才需要物理上连续的内存块，但是很多内核代码都调用kmalloc(), 而不是vmalloc()获得内存。这主要是出于性能的考虑，vmalloc()函数为了把物理上不连续的页面转换为虚拟地址空间上的连续的页，必须建立页表才行。还有通过vmalloc()获得的页必须一个个的进行映射（因为它们物理上不是连续的），这就会导致比直接内存映射大得多的缓冲区刷新。因为这些原因，vmalloc()仅在绝对必要时才会使用——典型的就是为了获得大块内存时，例如，当模块被动态插入到内核中时，就把模块装在到由vmalloc()分配的内存上。

下面的图总结了上述两种用户空间和内核空间虚拟内存分配方式。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f72fdf663df045fbbe3a4f4177b5517d~tplv-k3u1fbpfcp-zoom-1.image)

# 总结

本文首先回顾了内存虚拟化技术，介绍了其中的分段、分页具体原理；再描述了地址空间，其中包括用户空间和内核空间的划分；最后介绍了Linux物理页框管理、内存区管理和非连续内存区管理的内容。

操作系统中的很多思想和经典的算法，我们都可以在日常开发使用的各种工具或者框架中找到它们的影子，例如利用局部性原理缓存优化，batch移动CPU空闲对象等。Linux内存管理是一个非常复杂的系统，本文所述只是冰山一角，从宏观角度展现内存管理的全貌。当然也希望大家能够通过阅读了解更深层次的原理。

推荐书籍：

-   《操作系统导论》
-   《深入理解Linux内核》